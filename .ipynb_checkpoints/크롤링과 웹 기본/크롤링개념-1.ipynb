{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "잔금대출에도 DTI 규제 적용 검토\n"
     ]
    }
   ],
   "source": [
    "import requests  # 1. 라이브러리 임포트 \n",
    "from bs4 import BeautifulSoup\n",
    "res = requests.get('http://v.media.daum.net/v/20170615203441266') # 2. 웹페이지 가져오기 \n",
    "soup = BeautifulSoup(res.content,'html.parser')# 3. 웹페이지 파싱하기 \n",
    "mydata = soup.find('title')# 4. 필요한 데이터 추출하기 \n",
    "print(mydata.get_text()) # 5. 추출한 데이터 활용하기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 필요 라이브러리 임포트\n",
    "* 1. request: 웹페이지 가져오기 라이브러리 \n",
    "* 2. bs4(BeautifulSoup): 가져온 웹페이지를 기반으로 해서 특정 데이터를 추출할 수 있는 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup # bs4 라이브러리에서 BeautifulSoup 일종의 함수, 클래스를 가져오겠다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=requests.get(\"http://v.media.daum.net/v/20170615203441266\") # 웹페이지에서 가져온 결과값을 res 변수에 담는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IP Address: ex) 123.123.11.51\n",
    "* 내가 다른 컴퓨터가 가지고 있는 정보에 접속을 하고 싶으면, IP 주소를 입력하면 접속할 수 있다. \n",
    "* -> IP주소는 어려우니, WEB주소를 입력하면 자동으로 IP주소로 변환이 되서 접속가능하다. \n",
    "* 요청을 받으면 반환 값으로 HTML 파일을 반환하여 웹 안에서 화면을 띄우게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 웹페이지 가져오기\n",
    "* res.content 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=requests.get(\"http://v.media.daum.net/v/20170615203441266\")\n",
    "# IP 주소 혹은 웹 주소를 요청해서 반환 받은 html 문서를 res 변수에 담는다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 웹페이지 파싱하기\n",
    "* 파싱이란? 문자열의 의미를 분석하는 행위"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-e855e6333e48>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-14-e855e6333e48>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    <html>\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 이것도 일종의 문자열이다! \n",
    "\n",
    "<html>\n",
    "    <head>\n",
    "    </head>\n",
    "    <body>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### soup 에 HTML 파일을 파싱한 정보가 들어감!\n",
    "* BeautifulSoup 라이브러리를 활용!\n",
    "##### soup = BeautifulSoup(res.content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.content, 'html.parser') #첫번째 인자: 분석할 재료, 두번째 인자: 'html.parser'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 soup 변수 안에는 res.content를 구조적으로 분석된 데이터가 들어가게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. *** 가장 중요! 필요한 데이터 추출하기\n",
    "\n",
    "* soup.find()함수로 원하는 부분을 지정하면 된다. \n",
    "* 변수.get_text()함수로 추출한 부분을 가져올 수 있다. \n",
    "* ex) mydata=soup.find('title')\n",
    "* print(mydata.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata= soup.find('h3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTML 언어로 어떻게 웹페이지를 만드는지 이해할 필요가 있음!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "잔금대출에도 DTI 규제 적용 검토\n"
     ]
    }
   ],
   "source": [
    "print(mydata.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[주간리그컵] 역시 아스날의 넘버1은 레노였다!! 선방쇼 덕분에 8강 진출 - YouTube\n"
     ]
    }
   ],
   "source": [
    "res_1=requests.get('https://www.youtube.com/watch?v=OKJdwVeZGqM')\n",
    "soup_1 = BeautifulSoup(res_1.content, 'html.parser')\n",
    "mydata_1 = soup_1.find('title')\n",
    "print(mydata_1.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1=requests.get('https://www.youtube.com/watch?v=OKJdwVeZGqM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_1 = BeautifulSoup(res_1.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata_1 = soup_1.find('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>[주간리그컵] 역시 아스날의 넘버1은 레노였다!! 선방쇼 덕분에 8강 진출 - YouTube</title>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[주간리그컵] 역시 아스날의 넘버1은 레노였다!! 선방쇼 덕분에 8강 진출 - YouTube\n"
     ]
    }
   ],
   "source": [
    "print(mydata_1.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 패턴 총정리! \n",
    "\n",
    "### 1. 라이브러리 임포트 \n",
    "* import requests\n",
    "* from bs4 import BeautifulSoup\n",
    "\n",
    "### 2. 웹페이지 가져오기\n",
    "* res=request.get('웹 페이지 주소')\n",
    "\n",
    "### 3. 웹페이지에서 가져온 정보 파싱하기\n",
    "* soup =BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "### 4. 파싱한 정보를 기반으로 필요한 데이터 찾는 코드\n",
    "* mydata = soup.find('title')\n",
    "\n",
    "### 5. 추출한 데이터를 변수에 넣은 후 원하는 프로그래밍\n",
    "* print(mydata.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 크롤링 연습하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "행복한숲어린이집\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "res_5 = requests.get('http://happyforest.kidis.co.kr/') #웹페이지 불러오기 \n",
    "a= BeautifulSoup(res_5.content,'html.parser') #웹페이지 파싱\n",
    "b= a.find('title') #파싱한 정보에서 title 부분을 추출하기 \n",
    "print(b.get_text()) #get_text()로 <title> ~~~ </title> 내에 있는 내용 출력하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
